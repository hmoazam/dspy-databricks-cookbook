{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3d20f99-8ee8-4ffc-a569-7f971cf09ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26ff1de3-57c8-45e3-acee-89471c333da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Effective use of DSPy involves evaluation and iterative development\n",
    "\n",
    "You already know a lot about DSPy at this point. If all you want is quick scripting, you likely now have the skillset to be effective. Sprinkling DSPy signatures and modules into your Python control flow is a pretty ergonomic way to just get stuff done with LMs.\n",
    "\n",
    "That said, you're likely here because you want to build high-quality systems and improve them over time. The way to do that in DSPy is to leverage an evaluation cycle along with DSPy's [Optimizers](https://dspy.ai/learn/optimization/overview/).\n",
    "\n",
    "For our prototype we will use a bunch of StackExchange-based questions and their correct answers from the [RAG-QA Arena](https://arxiv.org/abs/2407.13998) dataset.  The dataset is prepare and divided into a train, validation, and dev sub-sets in the notebook executed below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69022c7f-f156-4f7b-9c3a-4c4e776f459d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: prep data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0d9b01a-ce7f-4de3-8c8c-9f6471cda7c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluation in DSPy\n",
    "\n",
    "What kind of metrics suit our question-answering task? There are many choices, but since the answers are long, we may ask: How well does the system response _cover_ all key facts in the gold response? And the other way around, how well is the system response _not saying things_ that aren't in the gold response?\n",
    "\n",
    "The above definition is essentially a **semantic F1**, so let's load a `SemanticF1` metric from DSPy. This metric is actually implemented as a [very simple DSPy module](https://github.com/stanfordnlp/dspy/blob/77c2e1cceba427c7f91edb2ed5653276fb0c6de7/dspy/evaluate/auto_evaluation.py#L21), which makes it usable with any DSPy LM.\n",
    "\n",
    "The metric is calculated based on:\n",
    "<br>\n",
    "$$Semantic F1 = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "$${\\text{Precission}}: {\\text{fraction (out of 1.0) of system response covered by the ground truth}}$$\n",
    "$${\\text{Recall}}: {\\text{fraction (out of 1.0) of ground truth covered by the system response}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c3b3c71-e53c-48e5-b588-779939d144eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dspy.evaluate import SemanticF1\n",
    "\n",
    "# Let's pick an `example` here from the data.\n",
    "example = trainset[0]\n",
    "# Produce a prediction from our `cot` module, using the `example` above as input.\n",
    "pred = cot(**example.inputs())\n",
    "metric = SemanticF1()\n",
    "score = metric(example, pred) #measure_correctness(example, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dd3677a-de59-4ffc-8688-51d7e8297721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For evaluation, you could use the metric above in a simple loop and just average the score. But for parallelism some additional utilities, we can rely on `dspy.Evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32bc6051-06aa-499b-abda-1cfbec87e5ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define an evaluator that we can re-use.\n",
    "evaluate = dspy.Evaluate(\n",
    "    devset=devset,\n",
    "    metric=metric, \n",
    "    num_threads=32,\n",
    "    display_progress=True,\n",
    "    display_table=2,\n",
    ")\n",
    "\n",
    "# Evaluate the Chain-of-Thought program.\n",
    "evaluate(cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4ec5681-fd6d-4ea4-b71c-fb2644995014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 4: DSPy Optimization and RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f903b52e-a9c8-4922-8964-32c7f3eb69f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "So far, we built a very simple chain-of-thought module for question answering and evaluated it on a small dataset, but can we do better?\n",
    "\n",
    "In the rest of this guide, we will build a retrieval-augmented generation (RAG) program in DSPy for the same task. We'll see how this can boost the score substantially, then we'll use one of the DSPy Optimizers to compile our RAG program to higher-quality prompts, raising our scores even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d9fe3c2-77fe-4c95-9aa5-5c0755dbcb27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Set up your system's retriever.\n",
    "As far as DSPy is concerned, you can plug in any Python code for calling tools or retrievers. Here, we'll use the Datbricks Vector Search index we set up earlier to execute a Hybrid Semantic Search (using embeddings and keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63ac654d-aab3-4a6f-b5b6-6c0a10b3de94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "import mlflow \n",
    "# from dspy.retrieve.databricks_rm import DatabricksRM\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_docs=5, for_mosaic_agent=False):\n",
    "\n",
    "        # setup mlflow tracing\n",
    "        mlflow.dspy.autolog()\n",
    "\n",
    "        # setup flag indicating if the object will be deploy as a Mosaic Agent\n",
    "        self.for_mosaic_agent = for_mosaic_agent\n",
    "        \n",
    "        # setup the language model\n",
    "        self.lm = dspy.LM(\"databricks/databricks-meta-llama-3-3-70b-instruct\")\n",
    "\n",
    "        # setup the predictor and signature\n",
    "        self.respond = dspy.ChainOfThought(\"context, question -> response\")\n",
    "\n",
    "        # setup the retriever pointing to Databricks Vector Search Index\n",
    "        self.retriever = DatabricksRM(\n",
    "            databricks_index_name=\"main.luis_moros.colbertv2_text_set_index\",\n",
    "            k=num_docs,\n",
    "            use_with_databricks_agent_framework=for_mosaic_agent\n",
    "        )\n",
    "\n",
    "    def forward(self, question):\n",
    "\n",
    "        if self.for_mosaic_agent:\n",
    "            question = question[-1][\"content\"]\n",
    "\n",
    "        context = self.retriever(\n",
    "            question, \n",
    "            query_type=\"hybrid\"# Using hybrid search (embeddigns + keywords search)\n",
    "        )\n",
    "\n",
    "        with dspy.context(lm=self.lm):\n",
    "            response = self.respond(context=context, question=question)\n",
    "\n",
    "        if self.for_mosaic_agent:\n",
    "            return response.response\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92ef446b-0690-4c7f-984a-127f7717afb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rag = RAG()\n",
    "rag(question=\"what are high memory and low memory on linux?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0be34867-d812-461f-aadb-fcca8d7f1304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluate(rag)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "04_dspy_basic_rag",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
