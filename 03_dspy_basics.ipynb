{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe7d4ac0-e4fc-42e9-87e2-a251ca058737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#DSPy + MLflow\n",
    "    \n",
    "Let's walk through a quick example of **basic question answering** with and without **retrieval-augmented generation** (RAG) in [DSPy](https://dspy.ai/). Specifically, let's build **a system for answering Tech questions**, e.g. about Linux or iPhone apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35b5c14b-3003-461b-ac86-53e81e8b1d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "324bd581-4442-4f18-bed1-faa04975b467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 1: Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42262836-5267-4750-a8ce-bb75e78485e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuring the DSPy environment.\n",
    "\n",
    "Let's configure DSPy to use the `meta-llama-3-1-3b-instruct` model we deployed during the setup. Our config file contains the `CHAT_MODEL_NAME` variable pointing to the deployed model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1938246-f01b-43a7-ad5b-1b297948c329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "lm = dspy.LM(CHAT_MODEL_NAME)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c203665-18e1-46ba-a829-877eeb878caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## GenAI Tracing via MLflow Autologging\n",
    "\n",
    "[MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) is a feature that enhances observability in Generative AI applications by capturing detailed information about the execution of services. It records inputs, outputs, and metadata for each step of a request, aiding in debugging and performance monitoring. Tracing can be automated with libraries like LangChain, OpenAI, and recently, DSPy! By simpliy using `mlflow.<library>.autolog()` you get detailed execution traces for you GenAI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68f6ff29-a54f-40bf-a8b3-1ab02747cc89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow \n",
    "mlflow.dspy.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f104bb05-cdee-445f-a157-e4bcc55d2381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 2: Example Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c10ebd7-1d00-486e-8ea6-9df6087263ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploring basic DSPy Modules\n",
    "\n",
    "There are several ways to call an LLM using dspy. The simplest is via `lm(prompt=\\\"prompt\\\")` or `lm(messages=[...])`, but this just sends the prompt/messages to the model and returns the response. \n",
    "\n",
    "DSPy also gives you `Modules` as a more robust way to define your LM functions.\n",
    "\n",
    "The simplest module is `dspy.Predict`. It takes a [DSPy Signature](https://dspy.ai/learn/programming/signatures/?h=signature), which is simply a structured input and output schema, and returns a callable for the behavior you specified. Let's use the `\\\"in-line\\\"` notation for signatures to declare a module that takes a question (`str`) and returns a `response` (`str`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e738faf-b2f3-4ff4-bcfc-a3934ac7c833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TODO: add details here on why dspy uses signatures instead of prompts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "752946d6-7337-44f2-b307-f8ea6b51bc81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define prediction strategy and signature\n",
    "qa = dspy.Predict('question: str -> response: str')\n",
    "# run inference\n",
    "response = qa(question=\"what are high memory and low memory on linux?\")\n",
    "\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d121b9aa-a230-422c-b514-f14166c3be67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As show above in the MLflow trace, we can see the variable names question and response in the signature defined our input and output, respectively.\n",
    "\n",
    "Now, what did DSPy do to build this qa module? Nothing fancy in this example, yet. The module passed your signature, LM, and inputs to an Adapter, which is a layer that handles structuring the inputs and parsing structured outputs to fit your plaintext signature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54b3a492-1084-434f-bcb3-6dd16af96569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Other DSPy Predictors (prediction strategies)\n",
    "DSPy has various built-in modules, e.g. `dspy.ChainOfThought`, `dspy.ProgramOfThought`, and `dspy.ReAct`. These are interchangeable with basic `dspy.Predict`: they take your signature, which is specific to your task, and they apply general-purpose prompting techniques and inference-time strategies.\n",
    "\n",
    "For example, `dspy.ChainOfThought` is an easy way to elicit _reasoning_ from your language model before it outputs the requested in your signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e51486c-779e-4c03-b5fb-ba86ecc43c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cot = dspy.ChainOfThought('question: str -> response: str')\n",
    "cot(question=\"what is Linux?\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_dspy_basics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
